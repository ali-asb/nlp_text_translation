{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "f-RbAz4Wyen1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure proper NLTK setup\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bea-5_xMyi31",
        "outputId": "12160494-4dc5-4052-c024-d6a709c6ec0e"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv('sentences.csv').dropna().drop_duplicates()"
      ],
      "metadata": {
        "id": "pSfaVMmwym8W"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "df['eng'] = df['eng'].apply(remove_punctuation)\n",
        "df['darija'] = df['darija'].apply(remove_punctuation)\n",
        "df = df[df['eng'].str.strip().astype(bool) & df['darija'].str.strip().astype(bool)]\n"
      ],
      "metadata": {
        "id": "OP_sGuqgypWW"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "SPLIT_SIZE = 0.2\n",
        "train_data, test_data = train_test_split(df, test_size=SPLIT_SIZE, random_state=4)\n",
        "train_data, val_data = train_test_split(train_data, test_size=SPLIT_SIZE, random_state=4)"
      ],
      "metadata": {
        "id": "wmWm1xhDyrOH"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_lowercase(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "X = train_data['eng'].apply(tokenize_and_lowercase).tolist()\n",
        "Y = train_data['darija'].apply(tokenize_and_lowercase).tolist()\n",
        "X_val = val_data['eng'].apply(tokenize_and_lowercase).tolist()\n",
        "Y_val = val_data['darija'].apply(tokenize_and_lowercase).tolist()\n",
        "X_test = test_data['eng'].apply(tokenize_and_lowercase).tolist()\n",
        "Y_test = test_data['darija'].apply(tokenize_and_lowercase).tolist()"
      ],
      "metadata": {
        "id": "X_AEfV21ywwB"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabularies\n",
        "vocab_eng = set(word for sentence in X for word in sentence)\n",
        "vocab_dari = set(word for sentence in Y for word in sentence)\n",
        "\n",
        "word_to_ix_eng = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "word_to_ix_eng.update({word: i + len(word_to_ix_eng) for i, word in enumerate(vocab_eng)})\n",
        "\n",
        "word_to_ix_dari = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "word_to_ix_dari.update({word: i + len(word_to_ix_dari) for i, word in enumerate(vocab_dari)})"
      ],
      "metadata": {
        "id": "AM4xug86yyf8"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sequences\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    return torch.tensor(\n",
        "        [to_ix['<SOS>']] + [to_ix.get(word, to_ix['<UNK>']) for word in seq] + [to_ix['<EOS>']],\n",
        "        dtype=torch.long\n",
        "    )\n",
        "\n",
        "X_train_encoded = [prepare_sequence(seq, word_to_ix_eng) for seq in X]\n",
        "Y_train_encoded = [prepare_sequence(seq, word_to_ix_dari) for seq in Y]\n",
        "X_val_encoded = [prepare_sequence(seq, word_to_ix_eng) for seq in X_val]\n",
        "Y_val_encoded = [prepare_sequence(seq, word_to_ix_dari) for seq in Y_val]"
      ],
      "metadata": {
        "id": "mehfcZGdy7rh"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences\n",
        "X_train_padded = pad_sequence(X_train_encoded, batch_first=True, padding_value=word_to_ix_eng['<PAD>'])\n",
        "Y_train_padded = pad_sequence(Y_train_encoded, batch_first=True, padding_value=word_to_ix_dari['<PAD>'])\n",
        "X_val_padded = pad_sequence(X_val_encoded, batch_first=True, padding_value=word_to_ix_eng['<PAD>'])\n",
        "Y_val_padded = pad_sequence(Y_val_encoded, batch_first=True, padding_value=word_to_ix_dari['<PAD>'])\n"
      ],
      "metadata": {
        "id": "lYZiVw8oy97s"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = TensorDataset(X_train_padded, Y_train_padded)\n",
        "val_dataset = TensorDataset(X_val_padded, Y_val_padded)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "7-R-z471y_io"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file, vocab, embedding_dim):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)\n",
        "            embeddings[word] = vector\n",
        "\n",
        "    embedding_matrix = torch.zeros(len(vocab), embedding_dim)\n",
        "    for word, idx in vocab.items():\n",
        "        if word in embeddings:\n",
        "            embedding_matrix[idx] = embeddings[word]\n",
        "        else:\n",
        "            embedding_matrix[idx] = torch.randn(embedding_dim)  # Random for unknown words\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "ESNKEsMPzBiV"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# File path to GloVe\n",
        "GLOVE_PATH = \"glove.6B.100d.txt\"\n",
        "EMBEDDING_DIM = 100  # GloVe embedding size\n",
        "\n",
        "embedding_matrix = load_glove_embeddings(GLOVE_PATH, word_to_ix_eng, EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "0ifg1KQpzE91"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CustomLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Weight matrices for input, forget, and output gates\n",
        "        self.Wxi = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.Wci = nn.Parameter(torch.zeros(hidden_size))  # Peephole connection\n",
        "\n",
        "        self.Wxf = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.Wcf = nn.Parameter(torch.zeros(hidden_size))  # Peephole connection\n",
        "\n",
        "        self.Wxo = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.Wco = nn.Parameter(torch.zeros(hidden_size))  # Peephole connection\n",
        "\n",
        "        # Weight matrices for candidate memory (g_t)\n",
        "        self.Wxg = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        # Bias terms\n",
        "        self.bi = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.bf = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.bo = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.bc = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        h_prev, c_prev = hidden\n",
        "\n",
        "        # Compute candidate memory (g_t)\n",
        "        g_t = torch.tanh(self.Whg(h_prev) + self.Wxg(x) + self.bc)\n",
        "\n",
        "        # Compute input gate (i_t) with working memory connections\n",
        "        i_t = torch.sigmoid(self.Wxi(x) + self.Whi(h_prev) + torch.tanh(self.Wci * c_prev) + self.bi)\n",
        "\n",
        "        # Compute forget gate (f_t) with working memory connections\n",
        "        f_t = torch.sigmoid(self.Wxf(x) + self.Whf(h_prev) + torch.tanh(self.Wcf * c_prev) + self.bf)\n",
        "\n",
        "        # Compute new cell state (c_t)\n",
        "        c_t = f_t * c_prev + i_t * g_t\n",
        "\n",
        "        # Compute output gate (o_t) with working memory connections\n",
        "        o_t = torch.sigmoid(self.Wxo(x) + self.Who(h_prev) + torch.tanh(self.Wco * c_t) + self.bo)\n",
        "\n",
        "        # Compute new hidden state (h_t)\n",
        "        h_t = o_t * torch.tanh(g_t)\n",
        "\n",
        "        return h_t, c_t"
      ],
      "metadata": {
        "id": "J7LnX2GR6pZE"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, embedding_matrix, bidirectional=False, dropout=0.3):\n",
        "        super(AdvancedLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        # Initialize embeddings with pretrained weights\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "\n",
        "        # Custom LSTM cell for forward and backward passes\n",
        "        self.lstm_cell = CustomLSTMCell(embedding_matrix.size(1), hidden_size)\n",
        "        if bidirectional:\n",
        "            self.lstm_cell_reverse = CustomLSTMCell(embedding_matrix.size(1), hidden_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_size * self.num_directions, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # Embedding lookup\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Initialize outputs\n",
        "        outputs = []\n",
        "        h_t, c_t = hidden\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        for t in range(seq_len):\n",
        "            h_t, c_t = self.lstm_cell(embedded[:, t, :], (h_t, c_t))\n",
        "            outputs.append(h_t.unsqueeze(1))  # Shape: [batch_size, 1, hidden_size]\n",
        "\n",
        "        if self.bidirectional:\n",
        "            h_t_rev, c_t_rev = hidden\n",
        "            outputs_reverse = []\n",
        "            for t in reversed(range(seq_len)):\n",
        "                h_t_rev, c_t_rev = self.lstm_cell_reverse(embedded[:, t, :], (h_t_rev, c_t_rev))\n",
        "                outputs_reverse.insert(0, h_t_rev.unsqueeze(1))  # Prepend for reverse order\n",
        "\n",
        "            # Concatenate forward and backward outputs along the hidden dimension\n",
        "            outputs = [torch.cat([fwd, bwd], dim=-1) for fwd, bwd in zip(outputs, outputs_reverse)]\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)  # Shape: [batch_size, seq_len, hidden_size * num_directions]\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.output_layer(outputs)  # Shape: [batch_size, seq_len, output_size]\n",
        "\n",
        "        return outputs, (h_t, c_t)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        h_0 = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "        c_0 = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "        return h_0, c_0\n",
        "\n"
      ],
      "metadata": {
        "id": "aJc0Em-TzG-k"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, Y_batch in data_loader:\n",
        "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "        hidden = model.init_hidden(X_batch.size(0), device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(X_batch, hidden)\n",
        "\n",
        "        # Truncate outputs and Y_batch to the same sequence length\n",
        "        seq_len = min(outputs.size(1), Y_batch.size(1))\n",
        "        outputs = outputs[:, :seq_len, :]\n",
        "        Y_batch = Y_batch[:, :seq_len]\n",
        "\n",
        "        # Reshape for loss computation\n",
        "        outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "        Y_batch = Y_batch.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(outputs, Y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "op73UoQ61gJ2"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in data_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "            hidden = model.init_hidden(X_batch.size(0), device)\n",
        "            outputs, _ = model(X_batch, hidden)\n",
        "\n",
        "            # Truncate outputs and Y_batch to the same sequence length\n",
        "            seq_len = min(outputs.size(1), Y_batch.size(1))\n",
        "            outputs = outputs[:, :seq_len, :]\n",
        "            Y_batch = Y_batch[:, :seq_len]\n",
        "\n",
        "            # Reshape for loss computation\n",
        "            outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "            Y_batch = Y_batch.contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(outputs, Y_batch)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "bPZtEZkh1jWG"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_SIZE = 512  # Increased hidden size\n",
        "input_size = len(word_to_ix_eng)\n",
        "output_size = len(word_to_ix_dari)\n",
        "DROPOUT = 0.3\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Pass the embedding matrix while initializing the model\n",
        "model = AdvancedLSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    output_size=output_size,\n",
        "    embedding_matrix=embedding_matrix,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word_to_ix_dari['<PAD>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "8yrUGnBe1loy"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\",f\"Train Loss: {train_loss:.4f}\",f\"Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-2l31SV2UEA",
        "outputId": "549c3981-5728-41ad-a2f6-768321987c6b"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Train Loss: 5.8306 Validation Loss: 5.5020\n",
            "Epoch 2/10 Train Loss: 5.0382 Validation Loss: 5.3230\n",
            "Epoch 3/10 Train Loss: 4.3026 Validation Loss: 5.2431\n",
            "Epoch 4/10 Train Loss: 3.4075 Validation Loss: 5.3398\n",
            "Epoch 5/10 Train Loss: 2.4755 Validation Loss: 5.5494\n",
            "Epoch 6/10 Train Loss: 1.7538 Validation Loss: 5.7472\n",
            "Epoch 7/10 Train Loss: 1.3132 Validation Loss: 5.8896\n",
            "Epoch 8/10 Train Loss: 1.0300 Validation Loss: 6.1013\n",
            "Epoch 9/10 Train Loss: 0.8237 Validation Loss: 6.1866\n",
            "Epoch 10/10 Train Loss: 0.6650 Validation Loss: 6.2123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"advanced_lstm_translation_model.pth\")"
      ],
      "metadata": {
        "id": "ZG-IhTEW3aHE"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "def test_model(model, data_loader, word_to_ix, ix_to_word, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, _ in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            hidden = model.init_hidden(X_batch.size(0), device)\n",
        "            outputs, _ = model(X_batch, hidden)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for output_seq in outputs:\n",
        "                translated_sentence = [\n",
        "                    ix_to_word[ix.item()] for ix in output_seq if ix.item() not in {word_to_ix['<PAD>'], word_to_ix['<SOS>'], word_to_ix['<EOS>']}\n",
        "                ]\n",
        "                predictions.append(\" \".join(translated_sentence))\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "Li8KuKVO8UEM"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create reverse mappings for decoding\n",
        "ix_to_word_eng = {ix: word for word, ix in word_to_ix_eng.items()}\n",
        "ix_to_word_dari = {ix: word for word, ix in word_to_ix_dari.items()}\n",
        "\n",
        "# Prepare test DataLoader\n",
        "X_test_encoded = [prepare_sequence(seq, word_to_ix_eng) for seq in X_test]\n",
        "Y_test_encoded = [prepare_sequence(seq, word_to_ix_dari) for seq in Y_test]\n",
        "X_test_padded = pad_sequence(X_test_encoded, batch_first=True, padding_value=word_to_ix_eng['<PAD>'])\n",
        "Y_test_padded = pad_sequence(Y_test_encoded, batch_first=True, padding_value=word_to_ix_dari['<PAD>'])\n",
        "test_dataset = TensorDataset(X_test_padded, Y_test_padded)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "kOTr7tMTCVzB"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "test_predictions = test_model(model, test_loader, word_to_ix_dari, ix_to_word_dari, device)\n",
        "\n",
        "# Print a few test results\n",
        "for i in range(50):\n",
        "    print(f\"Original: {' '.join(X_test[i])}\")\n",
        "    print(f\"Predicted: {test_predictions[i]}\")\n",
        "    print(f\"Actual: {' '.join(Y_test[i])}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX_DO8JmCiBY",
        "outputId": "035528b3-ea5a-4d1c-82ef-15130f208ac5"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: theres creepers everywhere\n",
            "Predicted: kayn wa7d sket\n",
            "Actual: kaynin chmakriya fin ma mchiti\n",
            "\n",
            "Original: eight books\n",
            "Predicted: tmnya tlktouba\n",
            "Actual: tmnya t lktouba\n",
            "\n",
            "Original: and working for you in particular\n",
            "Predicted: o chi 3la\n",
            "Actual: olkhdma 3ndak 3la wjah lkhosos\n",
            "\n",
            "Original: unless we ask them to bring some with them\n",
            "Predicted: mnghir nakhdo tlab chi chi\n",
            "Actual: ma3ada ila guelna lihom yjibo chwia m3ahom\n",
            "\n",
            "Original: he has\n",
            "Predicted: 3endo\n",
            "Actual: aando\n",
            "\n",
            "Original: is there an emergency button\n",
            "Predicted: wach kayn chi ra7a l2itisal\n",
            "Actual: wach kayn boton dyal tawari2\n",
            "\n",
            "Original: sorry i havent asked how your day was\n",
            "Predicted: sma7 liya wach lik\n",
            "Actual: sma7 liya masawltkch kidaz nhark\n",
            "\n",
            "Original: i dont want\n",
            "Predicted: mabghitch\n",
            "Actual: mabghitch\n",
            "\n",
            "Original: im worried about i seem to worry about everything\n",
            "Predicted: ana 7it fhadchi kayban 3la hadchi kolchi\n",
            "Actual: ana khayf mn ban liya ana khayf 3la kolchi\n",
            "\n",
            "Original: its stupid to be lost when we have a good map\n",
            "Predicted: jat mkllkha hadi akhir bzarba kan\n",
            "Actual: mn lghaba2 twadar mnin tkon 3andak kharita mazyana\n",
            "\n",
            "Original: im out of bread and its getting late\n",
            "Predicted: ana mn o dyal ou l7al\n",
            "Actual: t9ada lia lkhobz o mcha l7al\n",
            "\n",
            "Original: theres only one thing for it\n",
            "Predicted: kayna wahd 7aja dakchi\n",
            "Actual: kayna 7aja wa7da lih\n",
            "\n",
            "Original: how do you know that\n",
            "Predicted: kifach 3rfti tgoul\n",
            "Actual: bach 3refti hadchi\n",
            "\n",
            "Original: i have to get over my emotions\n",
            "Predicted: khasni n7eyyed m3a 3la\n",
            "Actual: khssni nnsa l a7asis dyali\n",
            "\n",
            "Original: i have dark circles under eyes\n",
            "Predicted: 3ndi joj ke7lin\n",
            "Actual: 3endi l8alat ssawda2\n",
            "\n",
            "Original: thirty five past\n",
            "Predicted: o sba3 9sam\n",
            "Actual: o sb3 9sam\n",
            "\n",
            "Original: i got it\n",
            "Predicted: 3ndi\n",
            "Actual: f8emt\n",
            "\n",
            "Original: dont get too close\n",
            "Predicted: matkhafch 7ta\n",
            "Actual: mat9rrbch bzaf\n",
            "\n",
            "Original: homie\n",
            "Predicted: tsnna\n",
            "Actual: l3chir\n",
            "\n",
            "Original: i always forget to drink water\n",
            "Predicted: dima dima 9ar3a lma lma lma\n",
            "Actual: dima kannsa nchreb lma\n",
            "\n",
            "Original: i need to buy a new pair of pants\n",
            "Predicted: 5assni nchri nchri chi jdid\n",
            "Actual: khasni nchri serwal jdid\n",
            "\n",
            "Original: i regret it\n",
            "Predicted: ndmt 3liha\n",
            "Actual: ana nadm 3la hadchi\n",
            "\n",
            "Original: you must have special cats if they drink brandy\n",
            "Predicted: nta 3ndna ndiro chi chi i9dro oula dyal\n",
            "Actual: aykono 3ndak mchach fchkel ila kano kaycharbo brandy\n",
            "\n",
            "Original: courteous\n",
            "Predicted: tsnna\n",
            "Actual: lah i3mmerha dar\n",
            "\n",
            "Original: our office is on the four hundredth floor and i have to take the stairs every day\n",
            "Predicted: lfamila f m3a f o o dyal o o khasni nkhdam tri9\n",
            "Actual: lbiro dyalna kayn ftaba9 rba3mia okhasni ntla3 droj kolla nhar\n",
            "\n",
            "Original: i live near the sea\n",
            "Predicted: kan3ich f l\n",
            "Actual: kansken 7da lb7er\n",
            "\n",
            "Original: the farmer poisoned the rats\n",
            "Predicted: moul 7da dial 3la\n",
            "Actual: lflla7 semmem ttobbat\n",
            "\n",
            "Original: dont be nervous\n",
            "Predicted: matkounch mat3esseb\n",
            "Actual: mat3esebch\n",
            "\n",
            "Original: thats right\n",
            "Predicted: bsah\n",
            "Actual: bssa7\n",
            "\n",
            "Original: im aware that the problem requires a thorough reflection\n",
            "Predicted: ana m9tane3 bli ila mochkil dyalha dial dyal dyal tlyouma\n",
            "Actual: ana 3araf bli lmochkila aykhasha tafkir 3ami9\n",
            "\n",
            "Original: there are several hundred employees in this company cant i know the location of everyones office\n",
            "Predicted: kayn bnadm dyal dyal 3am o fhad lli li f l dial dyal\n",
            "Actual: kayn lmi2at dial nass kheddamin f had charika wach momkin n3rf fine ja l bureau dial kola wa7d\n",
            "\n",
            "Original: how old is your brother\n",
            "Predicted: ch7al f\n",
            "Actual: ch7al f3mer 5ouk\n",
            "\n",
            "Original: my teacher keeps telling me that i sound like im translating word by word\n",
            "Predicted: lwalida dyali ki liya dak ana ana dyal nta la sa3a wla\n",
            "Actual: lostad diali dima kaygoulia annani kanhder b7ala kanterjem kelma b kelma\n",
            "\n",
            "Original: we will have fun too\n",
            "Predicted: kolna ykon ddiyaf\n",
            "Actual: ghadi ntmat3o 7ta 7na\n",
            "\n",
            "Original: do you speak the language\n",
            "Predicted: wach kat2amen t8der\n",
            "Actual: wach kat8dar llogha\n",
            "\n",
            "Original: i used to cry a lot\n",
            "Predicted: wllft anani nbki bzaf\n",
            "Actual: knt kanbki bzaf\n",
            "\n",
            "Original: i dont have sibllings\n",
            "Predicted: ma3ndich ttelj\n",
            "Actual: ma3ndich khkhouti\n",
            "\n",
            "Original: when will you get paid\n",
            "Predicted: fo9ach ghadi\n",
            "Actual: imta ghattkhelles\n",
            "\n",
            "Original: excuse me i want to go to bed\n",
            "Predicted: wach lia bghit nmchi\n",
            "Actual: sme7 li 5anmchi n3ess\n",
            "\n",
            "Original: isnt the moss still supposed to grow north\n",
            "Predicted: wach ikono 9adr kayn l f\n",
            "Actual: wach makhasch ta7alib inodo f chamal\n",
            "\n",
            "Original: i am spent\n",
            "Predicted: ana mrid\n",
            "Actual: ana t9adit\n",
            "\n",
            "Original: have you been in agadir\n",
            "Predicted: wach knti f f\n",
            "Actual: fayt knti f agadir\n",
            "\n",
            "Original: how do you like to celebrate your birthday\n",
            "Predicted: kifach katbghi l3chiya les 3id milad\n",
            "Actual: kifach kay3ejbek tdewez 3id miladek\n",
            "\n",
            "Original: i will never cheat on you\n",
            "Predicted: ma3mmerni n9dr m3ak\n",
            "Actual: ma3emmri nkhonek\n",
            "\n",
            "Original: heres your sandwich\n",
            "Predicted: hak femmek\n",
            "Actual: hahoa sandwich dyalk\n",
            "\n",
            "Original: well now i cant relax knowing youre unhappy\n",
            "Predicted: wakha rah man9derch wach 7ta maghadich nta\n",
            "Actual: iwa daba maymkench nrta7 wana 3arfk ma rach9ach lik\n",
            "\n",
            "Original: remember when we thought high school was the toughest thing ever\n",
            "Predicted: 39lti mli kan kis7ab dima dima kan\n",
            "Actual: 39lti fach ka kis7ab lina lycee houa as3ab 7aja\n",
            "\n",
            "Original: wait\n",
            "Predicted: tsnna\n",
            "Actual: tsnnaw\n",
            "\n",
            "Original: he cant walk\n",
            "Predicted: kay3oum\n",
            "Actual: makay9derch ytmcha\n",
            "\n",
            "Original: im sorry im just not used to being in a plane\n",
            "Predicted: sm7 lia liya ghir la nkon f tyara\n",
            "Actual: sme7 lia ghir mamwellefch nrkeb f tiyara\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzNhv5gJCk1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}